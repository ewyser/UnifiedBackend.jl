var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Public-API","page":"API Reference","title":"Public API","text":"","category":"section"},{"location":"api/#Backend-Access","page":"API Reference","title":"Backend Access","text":"","category":"section"},{"location":"api/#Device-Selection","page":"API Reference","title":"Device Selection","text":"","category":"section"},{"location":"api/#Backend-Setup","page":"API Reference","title":"Backend Setup","text":"","category":"section"},{"location":"api/#Device-Management","page":"API Reference","title":"Device Management","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#UnifiedBackend.get_backend","page":"API Reference","title":"UnifiedBackend.get_backend","text":"get_backend() -> Backend\n\nGet the global backend configuration instance.\n\nReturns the singleton Backend instance that manages the execution platforms and library registry for the entire UnifiedBackend session.\n\nReturns\n\nBackend: The global backend configuration object\n\nExamples\n\nusing UnifiedBackend\n\n# Access the global backend\nb = get_backend()\n\n# Inspect available platforms\nprintln(b.exec.functional)\n\n# Access host devices\nprintln(keys(b.exec.host))\n\n# Access device (GPU) configurations\nprintln(keys(b.exec.device))\n\nSee Also\n\nBackend: Backend configuration structure\nExecutionPlatforms: Execution platform registry\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.Host","page":"API Reference","title":"UnifiedBackend.Host","text":"Host\n\nStruct representing a CPU (host) device configuration.\n\nFields\n\ncategory::Symbol: Device category (:cpu)\nplatform::Symbol: Platform identifier (e.g., :CPU)\nbrand::String: Manufacturer brand (e.g., \"Intel\", \"AMD\")\nname::String: Specific device model name\nBackend::Any: KernelAbstractions backend instance\nwrapper::Any: Array wrapper type (e.g., Array)\nhandle::Any: Device handle (unused for CPU)\n\nExample\n\nHost(brand=\"AMD\", name=\"AMD Ryzen 7 9800X3D\", Backend=CPU())\n\n\n\n\n\n","category":"type"},{"location":"api/#UnifiedBackend.Device","page":"API Reference","title":"UnifiedBackend.Device","text":"Device\n\nStruct representing an accelerator (GPU) device configuration.\n\nFields\n\ncategory::Symbol: Device category (:gpu)\nplatform::Symbol: Platform identifier (e.g., :CUDA, :ROCm)\nbrand::String: Manufacturer brand (e.g., \"NVIDIA\", \"AMD\")\nname::String: Specific device model name\nBackend::Any: KernelAbstractions backend instance\nwrapper::Any: Array wrapper type (e.g., CuArray, ROCArray)\nhandle::Any: Device handle for direct device manipulation\n\nExample\n\nDevice(brand=\"NVIDIA\", name=\"RTX 4090\", Backend=CUDA())\n\n\n\n\n\n","category":"type"},{"location":"api/#UnifiedBackend.Backend","page":"API Reference","title":"UnifiedBackend.Backend","text":"Backend\n\nImmutable struct representing the unified backend system configuration.     host  ::Hosts = Hosts()     device::Devices = Devices() It provides an immutable container for library tracking and execution platform management. The structure itself is immutable, but the contained dictionaries and ExecutionPlatforms are mutable, allowing runtime updates to device configurations.\n\nFields\n\nlib::Dict{String,Any}: Library and module registry for loaded components and file tracking\nexec::ExecutionPlatforms: Mutable execution platform configuration for host and device backends\n\nDesign Pattern\n\nThe Backend struct uses an immutable wrapper around mutable data pattern:\n\nThe struct itself cannot be reassigned, ensuring singleton integrity\nThe lib and exec fields can be mutated internally for runtime configuration updates\nThis provides both safety and flexibility\n\nExamples\n\nusing UnifiedBackend\n\n# Access the global backend instance\nb = get_backend()\n\n# Inspect library registry (populated by module loading system)\nprintln(keys(b.lib))\n\n# Access execution platforms\nprintln(b.exec.functional)\n\n# Get CPU devices\ncpus = b.exec.host\nfor (dev_id, config) in cpus\n    println(\"$dev_id: $(config[:name])\")\nend\n\n# Create a custom backend (typically not needed - use global instance)\ncustom_backend = Backend(\n    lib = Dict{String,Any}(\"custom\" => \"data\"),\ncpus = b.exec.host\n)\n\nGlobal Instance\n\nUnifiedBackend maintains a global singleton instance accessed via get_backend():\n\nb = get_backend()\ndevices = select_execution_backend(b.exec, \"host\")\n\nSee Also\n\nExecutionPlatforms: Execution platform registry structure\nget_backend: Accessor for the global Backend instance\nselect_execution_backend: Device selection function\n\n\n\n\n\n","category":"type"},{"location":"api/#UnifiedBackend.ExecutionPlatforms","page":"API Reference","title":"UnifiedBackend.ExecutionPlatforms","text":"ExecutionPlatforms\n\nMutable struct representing the execution platform configuration for host (CPU) and device (GPU).\n\nThis structure maintains the registry of available computational devices and their configurations. It is designed to be populated at runtime as backends are initialized and devices are detected.\n\nFields\n\nfunctional::Vector{String}: List of successfully initialized execution platforms with status messages\nhost::Dict{Symbol,Host}: Host (CPU) device configurations, indexed by device symbols (:dev1, :dev2, etc.)\ndevice::Dict{Symbol,Device}: Accelerator (GPU) device configurations, indexed by device symbols\n\nExample\n\nusing UnifiedBackend\n\n# Create a new execution platforms registry\nexec = ExecutionPlatforms()\n\n# Populate with CPU backend\nadd_backend!(exec, Val(:x86_64))\n\n# Check what's available\nprintln(exec.functional)  # [\"Available execution platform(s):\", \"✓ Intel x86_64\"]\n\n# Access first CPU device configuration\ncpu_dev = exec.host[:dev1]\nprintln(cpu_dev.name)      # \"Intel(R) Core(TM) i9-9900K...\"\nprintln(cpu_dev.platform)  # :CPU\nprintln(cpu_dev.Backend)   # CPU()\n\n# After loading CUDA\nusing CUDA\n# GPU devices automatically registered in exec.device\ngpu_dev = exec.device[:dev1]\nprintln(gpu_dev.brand)     # \"NVIDIA\"\n\nSee Also\n\nBackend: Top-level configuration containing an ExecutionPlatforms instance\nadd_backend!: Function to populate this structure with available devices\n\n\n\n\n\n","category":"type"},{"location":"api/#UnifiedBackend.select_execution_backend","page":"API Reference","title":"UnifiedBackend.select_execution_backend","text":"select_execution_backend(\n    bckd::ExecutionPlatforms, \n    select::String=\"host\"; \n    prompt::Bool=false, \n    distributed::Bool=false\n) -> NamedTuple\n\nHigh-level function to select execution backend (CPU or GPU) with automatic fallback.\n\nThis is the primary interface for backend selection in UnifiedBackend. It provides a simple string-based API to choose between host (CPU) and device (GPU) execution, with automatic fallback to CPU if no GPU is available.\n\nArguments\n\nbckd::ExecutionPlatforms: The execution platform registry\nselect::String=\"host\": Backend type - \"host\" for CPU, \"device\" for GPU\nprompt::Bool=false: Enable interactive device selection menu\ndistributed::Bool=false: Enable multi-device selection\n\nSelection Modes\n\nThree modes available via keyword arguments:\n\nDefault mode (prompt=false, distributed=false):\nReturns first available device\nNo user interaction\nBest for scripts and non-interactive use\nInteractive mode (prompt=true):\nShows radio menu for single device selection\nUser chooses one device from list\nIdeal for targeting specific hardware\nDistributed mode (distributed=true):\nShows multi-select checkbox menu\nUser chooses multiple devices\nFor parallel/distributed computations\n\nReturns\n\nNamedTuple of device configurations. Structure depends on selection:\n\nSingle device: (dev1 = Dict(...),)\nMultiple devices: (dev1 = Dict(...), dev2 = Dict(...), ...)\n\nGPU Fallback Behavior\n\nWhen select=\"device\":\n\nIf GPUs are available → returns selected GPU(s)\nIf no GPUs found → automatically falls back to CPU\nLogs the fallback for user awareness\n\nExamples\n\nusing UnifiedBackend\n\nb = backend()\n\n# === CPU Examples ===\n\n# Default: first CPU core\ncpu = select_execution_backend(b.exec, \"host\")\n\n# Interactive: choose specific core\ncpu = select_execution_backend(b.exec, \"host\", prompt=true)\n\n# Distributed: select multiple cores\ncpus = select_execution_backend(b.exec, \"host\", distributed=true)\n\n# === GPU Examples ===\n\nusing CUDA  # or AMDGPU\n\n# Default: first GPU\ngpu = select_execution_backend(b.exec, \"device\")\n\n# Interactive: choose specific GPU (multi-GPU systems)\ngpu = select_execution_backend(b.exec, \"device\", prompt=true)\n\n# Multi-GPU computation\ngpus = select_execution_backend(b.exec, \"device\", distributed=true)\n\n# === Practical Usage ===\n\n# Select backend and use in computation\ndevices = select_execution_backend(b.exec, \"device\")\nbackend_instance = devices.dev1[:Backend]\narray_type = devices.dev1[:wrapper]\n\n# Create array on selected backend\ndata = array_type(rand(1000, 1000))\n\nError Handling\n\nThrows ArgumentError if select is not \"host\" or \"device\".\n\nLogging\n\nLogs selection via @info:\n\n\"Using CPU backend (default mode)\"\n\"Using GPU backend (interactive mode)\"\n\"No GPU available, falling back to CPU backend\"\n\nSee Also\n\nget_host: Direct CPU device selection\nget_device: Direct GPU device selection\nExecutionPlatforms: Device registry structure\nBackend: Top-level configuration\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.get_host","page":"API Reference","title":"UnifiedBackend.get_host","text":"get_host(bckd::ExecutionPlatforms; prompt::Bool=false, distributed::Bool=false) -> NamedTuple\n\nSelect and return host (CPU) execution device(s) from the available configurations.\n\nThis function provides three selection modes:\n\nDefault: Returns the first CPU device (:dev1)\nInteractive (prompt=true): Shows a radio menu to select a single device\nDistributed (distributed=true): Shows a multi-select menu for multiple devices\n\nArguments\n\nbckd::ExecutionPlatforms: The execution platform registry containing host devices\nprompt::Bool=false: Enable interactive single-device selection\ndistributed::Bool=false: Enable multi-device selection\n\nReturns\n\nNamedTuple where:\n\nKeys are device symbols (:dev1, :dev2, etc.)\nValues are device configuration dictionaries\n\nExamples\n\nusing UnifiedBackend\n\nb = backend()\n\n# Get first CPU device (default)\ncpu = get_host(b.exec)\n# (dev1 = Dict(:name => \"Intel...\", :platform => :CPU, ...),)\n\n# Interactive selection - shows menu\ncpu = get_host(b.exec, prompt=true)\n\n# Multi-device selection - shows checkboxes\ncpus = get_host(b.exec, distributed=true)\n# (dev1 = Dict(...), dev3 = Dict(...), dev5 = Dict(...),)\n\n# Access device properties\nprintln(cpu.dev1[:name])     # \"Intel(R) Core(TM) i9-9900K...\"\nprintln(cpu.dev1[:Backend])  # CPU()\n\nInteractive Menus\n\nWhen prompt=true or distributed=true, displays terminal menus using REPL.TerminalMenus:\n\nUse arrow keys to navigate\nPress Enter to confirm (radio menu)\nUse Space to toggle, Enter to confirm (multi-select menu)\n\nSee Also\n\nget_device: GPU device selection\nselect_execution_backend: High-level backend selection\nExecutionPlatforms: Device registry structure\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.get_device","page":"API Reference","title":"UnifiedBackend.get_device","text":"get_device(bckd::ExecutionPlatforms; prompt::Bool=false, distributed::Bool=false) -> NamedTuple\n\nSelect and return accelerator (GPU) execution device(s) from the available configurations.\n\nThis function provides three selection modes for GPU devices:\n\nDefault: Returns the first GPU device\nInteractive (prompt=true): Shows a radio menu to select a specific GPU\nDistributed (distributed=true): Shows a multi-select menu for multiple GPUs\n\nArguments\n\nbckd::ExecutionPlatforms: The execution platform registry containing device (GPU) configurations\nprompt::Bool=false: Enable interactive single-device selection\ndistributed::Bool=false: Enable multi-device selection\n\nReturns\n\nNamedTuple where:\n\nKeys are device symbols (:dev1, :dev2, etc.)\nValues are GPU configuration dictionaries\n\nGPU Configuration\n\nEach GPU device contains:\n\n:dev: \"gpu\"\n:platform: :CUDA, :ROCm, etc.\n:brand: \"NVIDIA\", \"AMD\", etc.\n:name: Full GPU model name\n:Backend: KernelAbstractions backend (CUDABackend(), ROCBackend())\n:wrapper: Array type (CuArray, ROCArray)\n:handle: Native device handle (CuDevice, HIPDevice)\n\nExamples\n\nusing UnifiedBackend, CUDA\n\nb = backend()\n\n# Get first GPU (default)\ngpu = get_device(b.exec)\n# (dev1 = Dict(:name => \"NVIDIA GeForce RTX 3090\", :platform => :CUDA, ...),)\n\n# Interactive selection\ngpu = get_device(b.exec, prompt=true)\n\n# Multi-GPU selection\ngpus = get_device(b.exec, distributed=true)\n# (dev1 = Dict(...), dev2 = Dict(...),)\n\n# Access GPU properties\nprintln(gpu.dev1[:name])      # \"NVIDIA GeForce RTX 3090\"\nprintln(gpu.dev1[:platform])  # :CUDA\nprintln(gpu.dev1[:Backend])   # CUDABackend()\nprintln(gpu.dev1[:wrapper])   # CuArray\n\n# Use device handle for manual control\nCUDA.device!(gpu.dev1[:handle])\n\nRequirements\n\nGPU devices are only available after loading the appropriate package:\n\nNVIDIA: using CUDA\nAMD: using AMDGPU\n\nIf no GPUs are registered, this function will error. Use select_execution_backend for automatic fallback to CPU.\n\nSee Also\n\nget_host: CPU device selection\nselect_execution_backend: High-level backend selection with fallback\nExecutionPlatforms: Device registry structure\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.add_backend!","page":"API Reference","title":"UnifiedBackend.add_backend!","text":"add_backend!(bckd::ExecutionPlatforms, ::Val{ARCH}) where ARCH -> Nothing\n\nInitialize and populate the execution platform registry with host (CPU) backend configurations.\n\nThis generic implementation works for any CPU architecture by checking which backend in list_host_backend() is functional for the current system. The architecture-specific behavior is determined by the :functional flag in each backend configuration.\n\nTo add support for a new architecture, simply add an entry to list_host_backend().\n\nArguments\n\nbckd::ExecutionPlatforms: The execution platforms registry to populate\n::Val{ARCH}: Architecture specification (e.g., Val(:x86_64), Val(:aarch64))\n\nBehavior\n\nQueries system CPU information via Sys.cpu_info()\nMatches detected CPU brand against supported backends\nCreates a device entry for each logical CPU core\nUpdates the functional status log\nLogs initialization status via @info\n\nDevice Configuration\n\nEach registered device receives:\n\n:host: \"cpu\"\n:platform: :CPU\n:brand: Detected manufacturer (e.g., \"Intel(R)\", \"AMD\", \"Apple\")\n:name: Full CPU model string\n:Backend: KernelAbstractions CPU() backend\n:wrapper: Array type for computations\n:handle: nothing (CPUs don't require device handles)\n\nErrors\n\nThrows ErrorException if CPU model information cannot be retrieved from the system.\n\nSee Also\n\nExecutionPlatforms: The structure being populated\nlist_host_backend: Backend configuration specifications\nlist_cpu_devices: CPU device enumeration\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.list_host_backend","page":"API Reference","title":"UnifiedBackend.list_host_backend","text":"list_host_backend() -> Dict{Symbol, Dict{Symbol, Any}}\n\nReturn a dictionary of supported host (CPU) backend configurations for each architecture.\n\nEach architecture entry (e.g., :x86_64, :aarch64) contains:\n\n:host: Device category (\"cpu\")\n:Backend: KernelAbstractions CPU backend instance\n:brand: Array of supported manufacturer strings\n:wrapper: Array type for computations\n:devices: Placeholder for device list (usually nothing)\n:name: Placeholder for device name (usually nothing)\n:handle: Type handle (e.g., Val{:Host})\n:functional: Boolean indicating if this architecture matches the current system\n\nReturns\n\nDictionary mapping architecture symbols to configuration dictionaries.\n\nExamples\n\nbackends = list_host_backend()\nx86_config = backends[:x86_64]\nprintln(x86_config[:brand])      # [\"Intel(R)\", \"AMD\"]\nprintln(x86_config[:functional]) # true (on x86-64 systems)\nprintln(x86_config[:devices])    # nothing\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.list_cpu_devices","page":"API Reference","title":"UnifiedBackend.list_cpu_devices","text":"list_cpu_devices() -> Vector{String}\n\nGet a list of CPU device names from system information.\n\nQueries Sys.cpu_info() and extracts the model name prefix (before the colon) for each available CPU core. This is used to populate device configurations in the ExecutionPlatforms structure.\n\nReturns\n\nVector of CPU model name strings, one per logical core.\n\nExamples\n\ndevices = list_cpu_devices()\nprintln(length(devices))  # Number of logical CPU cores\nprintln(devices[1])       # \"Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\"\n\nSee Also\n\nSys.cpu_info(): Julia's system CPU information function\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.device_wakeup!","page":"API Reference","title":"UnifiedBackend.device_wakeup!","text":"device_wakeup!() -> Nothing\n\nStub function for activating a specific device.\n\nThis is a placeholder function that must be overloaded by backend extensions (CUDAExt, ROCmExt) to provide device-specific wake-up functionality. Device wake-up typically involves setting the active GPU context for subsequent operations.\n\nBackend Implementations\n\nExtensions should implement:\n\ndevice_wakeup!(::CuDevice) for CUDA GPUs\ndevice_wakeup!(::HIPDevice) for ROCm GPUs\n\nExamples\n\n# Will error - stub not overloaded\ndevice_wakeup!()  # ErrorException\n\n# After loading CUDA\nusing CUDA\ngpu = select_execution_backend(get_backend().exec, \"device\")\ndevice_wakeup!(gpu.dev1[:handle])  # Sets active CUDA device\n\nErrors\n\nThrows ErrorException if called without backend extension loaded.\n\nSee Also\n\nExtensions: CUDAExt, ROCmExt\nDevice selection: select_execution_backend\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedBackend.device_free!","page":"API Reference","title":"UnifiedBackend.device_free!","text":"device_free!(mesh, ::Val{:CPU}) -> Nothing\n\nFree device memory for CPU backend and trigger garbage collection.\n\nFor CPU execution, this function simply calls Julia's garbage collector to reclaim memory. GPU backend extensions may override this with device-specific memory management.\n\nArguments\n\nmesh: Any object whose memory should be freed (typically ignored for CPU)\n::Val{:CPU}: Type parameter specifying CPU backend\n\nExamples\n\n# Free CPU memory\ndata = rand(1000, 1000)\ndevice_free!(data, Val(:CPU))\n# Triggers GC.gc()\n\n# GPU backends would call device-specific free functions\n# device_free!(gpu_data, Val(:CUDA))  # Would call CUDA.unsafe_free!\n\nNotes\n\nThis is primarily useful for consistency with GPU backends. For CPU-only code, directly calling GC.gc() is equivalent.\n\nSee Also\n\nGC.gc(): Julia's garbage collector\nExtensions: CUDAExt, ROCmExt (provide GPU-specific implementations)\n\n\n\n\n\n","category":"function"},{"location":"#UnifiedBackend.jl","page":"Home","title":"UnifiedBackend.jl","text":"A unified backend abstraction layer for CPU and GPU execution in Julia.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"UnifiedBackend provides a consistent interface for managing and executing code across different computational backends (CPU, CUDA, ROCm) with automatic device detection and runtime configuration.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Automatic CPU detection: Supports x86_64 and aarch64 (Apple Silicon)\nGPU support: NVIDIA CUDA and AMD ROCm via package extensions  \nInteractive selection: Choose devices via terminal menus\nMulti-device: Run on multiple CPUs or GPUs\nType-stable: Uses KernelAbstractions.jl","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using UnifiedBackend\n\n# Access backend\nb = get_backend()\n\n# Use CPU\ncpu = select_execution_backend(b.exec, \"host\")\ndata = cpu.dev1[:wrapper](rand(1000, 1000))\n\n# Use GPU (with CUDA.jl loaded)\ngpu = select_execution_backend(b.exec, \"device\")\ndata = gpu.dev1[:wrapper](rand(1000, 1000))\n\nSee the API Reference and Extensions for more details.","category":"section"},{"location":"extensions/#Package-Extensions","page":"Extensions","title":"Package Extensions","text":"UnifiedBackend uses Julia's package extension system to provide optional GPU support. Extensions are automatically loaded when you install and load the corresponding GPU package.","category":"section"},{"location":"extensions/#Available-Extensions","page":"Extensions","title":"Available Extensions","text":"","category":"section"},{"location":"extensions/#CUDAExt-NVIDIA-GPU-Support","page":"Extensions","title":"CUDAExt - NVIDIA GPU Support","text":"Provides CUDA support for NVIDIA GPUs via CUDA.jl.","category":"section"},{"location":"extensions/#Installation","page":"Extensions","title":"Installation","text":"using Pkg\nPkg.add(\"CUDA\")","category":"section"},{"location":"extensions/#Usage","page":"Extensions","title":"Usage","text":"using UnifiedBackend\nusing CUDA  # Automatically loads CUDAExt\n\nb = get_backend()\n\n# Check for CUDA devices\nif !isempty(b.exec.device)\n    println(\"Found \", length(b.exec.device), \" CUDA GPU(s)\")\n    \n    # Select GPU\n    gpu = select_execution_backend(b.exec, \"device\")\n    \n    # Access CUDA-specific properties\n    println(\"GPU: \", gpu.dev1[:name])\n    println(\"Backend: \", gpu.dev1[:Backend])  # CUDABackend()\n    println(\"Array type: \", gpu.dev1[:wrapper])  # CuArray\n    \n    # Create data on GPU\n    data = gpu.dev1[:wrapper](rand(1000, 1000))\nend","category":"section"},{"location":"extensions/#Extended-Functions","page":"Extensions","title":"Extended Functions","text":"add_backend!(::Val{:CUDA}, ::Backend) - Detects and registers CUDA GPUs\ndevice_wakeup!(::CuDevice) - Activates a specific CUDA device\ndevice_free!(::Mesh, ::Val{:CUDA}) - Frees CUDA memory","category":"section"},{"location":"extensions/#ROCmExt-AMD-GPU-Support","page":"Extensions","title":"ROCmExt - AMD GPU Support","text":"Provides ROCm support for AMD GPUs via AMDGPU.jl.","category":"section"},{"location":"extensions/#Installation-2","page":"Extensions","title":"Installation","text":"using Pkg\nPkg.add(\"AMDGPU\")","category":"section"},{"location":"extensions/#Usage-2","page":"Extensions","title":"Usage","text":"using UnifiedBackend\nusing AMDGPU  # Automatically loads ROCmExt\n\nb = get_backend()\n\n# Check for ROCm devices\nif !isempty(b.exec.device)\n    println(\"Found \", length(b.exec.device), \" ROCm GPU(s)\")\n    \n    # Select GPU\n    gpu = select_execution_backend(b.exec, \"device\")\n    \n    # Access ROCm-specific properties\n    println(\"GPU: \", gpu.dev1[:name])\n    println(\"Backend: \", gpu.dev1[:Backend])  # ROCBackend()\n    println(\"Array type: \", gpu.dev1[:wrapper])  # ROCArray\n    \n    # Create data on GPU\n    data = gpu.dev1[:wrapper](rand(1000, 1000))\nend","category":"section"},{"location":"extensions/#Extended-Functions-2","page":"Extensions","title":"Extended Functions","text":"add_backend!(::Val{:ROCm}, ::Backend) - Detects and registers ROCm GPUs\ndevice_wakeup!(::HIPDevice) - Activates a specific ROCm device\ndevice_free!(::Mesh, ::Val{:ROCm}) - Frees ROCm memory","category":"section"},{"location":"extensions/#GPU-Fallback-Behavior","page":"Extensions","title":"GPU Fallback Behavior","text":"When requesting GPU execution without GPU extensions loaded, UnifiedBackend automatically falls back to CPU:\n\n# No GPU package loaded\ngpu = select_execution_backend(get_backend().exec, \"device\")\n# Info: No GPU available, falling back to CPU backend\n# Returns CPU device instead","category":"section"},{"location":"extensions/#Requirements","page":"Extensions","title":"Requirements","text":"","category":"section"},{"location":"extensions/#CUDA-Requirements","page":"Extensions","title":"CUDA Requirements","text":"Julia 1.9+\nNVIDIA GPU with CUDA support\nCUDA toolkit installed\nCUDA.jl package","category":"section"},{"location":"extensions/#ROCm-Requirements","page":"Extensions","title":"ROCm Requirements","text":"Julia 1.9+\nAMD GPU with ROCm support\nROCm runtime installed\nAMDGPU.jl package","category":"section"},{"location":"extensions/#Troubleshooting","page":"Extensions","title":"Troubleshooting","text":"If extensions fail to load, check:\n\nPackage installation:\n\nusing Pkg\nPkg.status()  # Verify CUDA/AMDGPU is installed\n\nGPU drivers:\n\nusing CUDA  # or AMDGPU\nCUDA.versioninfo()  # Check CUDA installation\nAMDGPU.versioninfo()  # Check ROCm installation\n\nExtension loading:\n\nusing UnifiedBackend\nget_backend()  # Check exec.functional for loaded backends\n\nWarnings during extension loading are captured and displayed with troubleshooting hints.","category":"section"}]
}
